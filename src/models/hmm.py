"""Hidden Markov Model for regime detection in cryptocurrency markets.

This module implements a Gaussian HMM-based approach to detect Risk-On/Risk-Off
regimes using statistical features from crypto and traditional market data.
"""

# Enable future annotations for cleaner type hints
from __future__ import annotations

# Use dataclass for clean configuration objects with automatic __init__ and __repr__
from dataclasses import dataclass

# Type hints for better code clarity and IDE support
from typing import Iterable, Optional, Tuple

# NumPy for numerical array operations
import numpy as np

# Pandas for time-series data manipulation with DatetimeIndex
import pandas as pd

# GaussianHMM from hmmlearn: implements Hidden Markov Models with Gaussian emissions
# This allows modeling of temporal dependencies in market regimes
from hmmlearn.hmm import GaussianHMM

# StandardScaler: normalizes features to zero mean and unit variance
# Critical for ML models to prevent features with large scales from dominating
from sklearn.preprocessing import StandardScaler


@dataclass
class HMMConfig:
    """Configuration for Hidden Markov Model regime detection.

    Attributes:
        n_states: Number of hidden states (regimes) to detect.
        covariance_type: Type of covariance parameters ('full', 'diag', 'spherical', 'tied').
        n_iter: Maximum number of EM iterations.
        tol: Convergence threshold for log-likelihood.
        random_state: Random seed for reproducibility.
        features: Optional tuple of feature column names to use. If None, uses all features.
    """

    # Number of hidden states (2 = Risk-On and Risk-Off regimes)
    # HMMs model observable data as being generated by hidden states
    n_states: int = 2

    # Covariance type determines how feature correlations are modeled
    # 'full' = each state has its own full covariance matrix (most flexible but more parameters)
    covariance_type: str = "full"

    # Maximum iterations for Baum-Welch (EM) algorithm
    # EM iteratively improves model parameters until convergence
    n_iter: int = 500

    # Convergence tolerance: stop when log-likelihood improvement < tol
    # Prevents unnecessary iterations once model has converged
    tol: float = 1e-3

    # Random seed for reproducible results across runs
    # Important for academic work to ensure experiments can be replicated
    random_state: int = 42

    # Optional feature subset selection
    # Allows testing different feature combinations without changing code
    features: Optional[Tuple[str, ...]] = None


def prepare_features(
    df: pd.DataFrame, feature_cols: Optional[Iterable[str]] = None
) -> Tuple[np.ndarray, pd.Index, StandardScaler, Tuple[str, ...]]:
    """Prepare and standardize features for HMM training.

    Args:
        df: DataFrame containing features with DatetimeIndex.
        feature_cols: Optional iterable of column names to use. If None, uses all columns
            except 'Date'.

    Returns:
        Tuple containing:
            - X: Standardized feature matrix (n_samples, n_features).
            - idx: DatetimeIndex of valid samples after dropping NaN.
            - scaler: Fitted StandardScaler for later transformation.
            - feature_cols: Tuple of feature column names used.

    Example:
        >>> features = pd.read_csv('features.csv', index_col=0, parse_dates=True)
        >>> X, idx, scaler, cols = prepare_features(features)
    """
    # If no specific features requested, use all columns except 'Date'
    # This provides flexibility: can use all features or a subset for feature selection
    if feature_cols is None:
        feature_cols = tuple(c for c in df.columns if c not in ("Date",))

    # Select only the requested feature columns and remove rows with missing values
    # dropna() is critical: HMM cannot handle NaN values in training data
    X_df = df.loc[:, feature_cols].dropna()

    # Preserve the DatetimeIndex for later alignment with regime predictions
    # This allows us to map regimes back to specific dates
    idx = X_df.index

    # Initialize StandardScaler to normalize features
    # Scaling ensures all features contribute equally regardless of their original scale
    scaler = StandardScaler()

    # Fit the scaler to data and transform in one step
    # fit_transform() computes mean/std and applies: (X - mean) / std
    # Result: each feature has mean=0 and std=1
    X = scaler.fit_transform(X_df.values)

    # Return: scaled features, date index, fitted scaler (for later use), and feature names
    return X, idx, scaler, tuple(feature_cols)


def fit_hmm(
    df: pd.DataFrame, cfg: Optional[HMMConfig] = None
) -> Tuple[GaussianHMM, pd.Index, StandardScaler, Tuple[str, ...]]:
    """Fit a Gaussian Hidden Markov Model to detect market regimes.

    Uses the Baum-Welch (EM) algorithm to learn hidden states from observable features.
    The model learns transition probabilities between regimes and emission distributions.

    Args:
        df: DataFrame with features and DatetimeIndex.
        cfg: HMMConfig object with model parameters. If None, uses defaults.

    Returns:
        Tuple containing:
            - model: Fitted GaussianHMM model.
            - idx: DatetimeIndex of training samples.
            - scaler: Fitted StandardScaler used for feature normalization.
            - feats: Tuple of feature names used in training.

    Example:
        >>> config = HMMConfig(n_states=2, covariance_type='full')
        >>> model, idx, scaler, feats = fit_hmm(features, config)
        >>> print(f"Trained on {len(idx)} samples with {len(feats)} features")
    """
    # Use provided config or create default HMMConfig
    # This pattern allows flexible configuration while providing sensible defaults
    cfg = cfg or HMMConfig()

    # Prepare and standardize features using the helper function
    # Returns scaled data, date index, scaler object, and feature names
    X, idx, scaler, feats = prepare_features(df, cfg.features)

    # Initialize Gaussian HMM with specified parameters
    # GaussianHMM assumes each hidden state emits observations from a Gaussian distribution
    model = GaussianHMM(
        # n_components = number of hidden states (regimes)
        n_components=cfg.n_states,
        # How to model covariance: 'full' allows each state to have different feature correlations
        covariance_type=cfg.covariance_type,
        # Maximum iterations for Baum-Welch (Expectation-Maximization) algorithm
        n_iter=cfg.n_iter,
        # Convergence threshold: stops when improvement in log-likelihood < tol
        tol=cfg.tol,
        # Random seed for reproducible initialization of parameters
        random_state=cfg.random_state,
    )

    # Fit the model using Baum-Welch algorithm
    # This learns: (1) transition matrix (regime switching probabilities)
    #              (2) emission parameters (mean and covariance for each regime)
    #              (3) initial state distribution
    model.fit(X)

    # Return trained model along with metadata needed for prediction
    return model, idx, scaler, feats


def decode_hmm(
    model: GaussianHMM,
    df: pd.DataFrame,
    scaler: StandardScaler,
    feature_cols: Iterable[str],
) -> Tuple[pd.Series, np.ndarray]:
    """Decode (predict) hidden regimes for given features using trained HMM.

    Uses the Viterbi algorithm to find the most likely sequence of hidden states
    given the observed features.

    Args:
        model: Trained GaussianHMM model.
        df: DataFrame with features to decode.
        scaler: Fitted StandardScaler from training.
        feature_cols: Feature column names to use (must match training features).

    Returns:
        Tuple containing:
            - regimes: Series with regime labels (0, 1, ..., n_states-1) indexed by date.
            - X: Standardized feature matrix used for prediction.

    Example:
        >>> regimes, X = decode_hmm(model, new_features, scaler, feature_cols)
        >>> print(f"Detected {regimes.nunique()} unique regimes")
        >>> print(regimes.value_counts())
    """
    # Extract the same features used during training
    # Must use identical feature columns to ensure consistency
    X_df = df.loc[:, feature_cols].dropna()

    # Preserve DatetimeIndex to align regime predictions with dates
    idx = X_df.index

    # Transform features using the SAME scaler fitted during training
    # Critical: use transform() not fit_transform() to apply training statistics
    # This ensures test data is scaled consistently with training data
    X = scaler.transform(X_df.values)

    # Use Viterbi algorithm to find most likely sequence of hidden states
    # Viterbi considers:
    # (1) emission probabilities (how likely each state generates features)
    # (2) transition probabilities (how likely to switch between states)
    # Result: globally optimal state sequence (not just independent predictions)
    z = model.predict(X)

    # Return regime labels as pandas Series with date index for easy analysis
    # Also return scaled features for potential further analysis
    return pd.Series(z, index=idx, name="regime"), X
